<h2>5.4 - Data Storage</h2>
<p><img src="images/case-study/storage.png" alt="Campfire's architecture with S3 highlighted"></p>
<p>The primary data that Campfire needed to store was session replay data, which took the form of arrays of event objects. For storage, we considered AWS Elastic File System (EFS) and AWS S3 buckets (S3), both accessible via lambdas and capable of being shared across all containers if needed.</p>
<p>Although both EFS and S3 would be able to support this data, they have slightly different use cases. EFS is meant to be a shared file system among multiple containers, and works well for when you need to structure your data in a hierarchical manner with nested folders and files. It’s primarily meant to be used internally by those instances.</p>
<p>S3 is an object storage system that allows you store any type of data in an unstructured way, and lets you retrieve it using a unique identifier. It’s useful as a data store for objects that need to be accessed over the internet. With S3, access to stored objects could be available outside the containerized instances.</p>
<p>Since Campfire didn’t necessarily need a shared file system, but rather a place to store discrete pieces of data to be retrieved later, S3 seemed a better fit. It also gave two more advantages:</p>
<ul>
<li>We could store our files for our Lambda functions in the S3 bucket, which Cloud Formation could then pull from during set-up</li>
<li>Opens up possibility for later storing user uploaded screenshots, which, because of the external nature of S3 items, would make the images available not just within the feedback interface, but also on github pull-requests</li>
</ul>
<p>The following diagram shows our architecture as a whole:
<img src="images/case-study/whole-architecture.png" alt="Campfire's overall architecture"></p>
